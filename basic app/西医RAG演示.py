# %%
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
#from langchain_chroma import Chroma
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 1. åŠ è½½å·²æœ‰å‘é‡åº“
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# 2. æ„å»º retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# 3. åˆå§‹åŒ– LLMï¼ˆQwen via DashScopeï¼‰
llm = ChatOpenAI(
    model="qwen-max",
    temperature=0.3,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  # ä¿®æ­£ï¼šå‚æ•°åæ˜¯ base_url
    api_key="sk-db85459561d04810ac504107dbd02936"
)

# 4. æ„å»º RAG é“¾ï¼ˆç°ä»£æ–¹å¼ï¼‰
basic_prompt = ChatPromptTemplate.from_messages([
    ("system", 
     "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„çš®è‚¤ç§‘ä¸´åºŠè¾…åŠ©è¯Šç–—ä¸“å®¶ï¼Œæ­£åœ¨è¾…åŠ©ä¸åŒèƒŒæ™¯çš„ç”¨æˆ·ç†è§£çš®è‚¤ç—…ç›¸å…³çŸ¥è¯†ã€‚"
     "ç”¨æˆ·å¯èƒ½æ˜¯åŒ»å­¦ç”Ÿã€ä½é™¢åŒ»å¸ˆã€èµ„æ·±ä¸­åŒ»ã€æ™®é€šæ‚£è€…æˆ–åŒ»å­¦çˆ±å¥½è€…ã€‚"
     "è¯·æ ¹æ®é—®é¢˜ä¸­éšå«çš„ä¸“ä¸šç¨‹åº¦ï¼Œè‡ªåŠ¨è°ƒæ•´å›ç­”çš„æ·±åº¦ä¸æœ¯è¯­ä½¿ç”¨ï¼š"
     "\n- è‹¥é—®é¢˜åŒ…å«ä¸“ä¸šæœ¯è¯­æˆ–æœºåˆ¶æ¢è®¨ï¼Œå¯ä½¿ç”¨è§„èŒƒåŒ»å­¦æœ¯è¯­ï¼Œå¹¶ç®€è¦è§£é‡Šå…³é”®æ¦‚å¿µï¼›"
     "\n- è‹¥é—®é¢˜åå‘ç—‡çŠ¶æè¿°æˆ–æ—¥å¸¸æŠ¤ç†ï¼Œè¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¿å… jargonï¼›"
     "\n- å§‹ç»ˆä¿æŒå°Šé‡ã€è€å¿ƒä¸åŒç†å¿ƒï¼Œä¸å‡è®¾ã€ä¸æ ‡ç­¾ç”¨æˆ·èº«ä»½ï¼›"
     "\n- åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä½œç­”ï¼Œè‹¥ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜â€œç°æœ‰èµ„æ–™è¾ƒå°‘ï¼Œæˆ‘å°†å°½æˆ‘æ‰€èƒ½ä¸ºä½ è§£é‡Šâ€ï¼Œå¹¶ä¸”æ ¹æ®ä½ çš„åŸæœ‰çŸ¥è¯†ä½œç­”ï¼›"
     "\n- å›ç­”éœ€ç®€æ´ï¼Œèšç„¦æ ¸å¿ƒä¿¡æ¯ï¼Œé¿å…å†—é•¿ï¼›"
     "\n- åœ¨å›ç­”æœ«å°¾ï¼Œç”¨å¼€æ”¾å¼æé—®å¼•å¯¼ç”¨æˆ·æ·±å…¥æ¢è®¨ï¼ˆå¦‚ï¼šâ€˜ä½ æ˜¯å¦è¿˜æƒ³äº†è§£å…¶é‰´åˆ«è¯Šæ–­ï¼Ÿâ€™ æˆ– â€˜éœ€è¦æˆ‘è§£é‡Šæ²»ç–—æ–¹æ¡ˆçš„ç»†èŠ‚å—ï¼Ÿâ€™ï¼‰"
    ),
    ("human", "å‚è€ƒèµ„æ–™ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}")
])

deep_prompt = ChatPromptTemplate.from_messages([
    ("system",
     "ä½ ç°åœ¨è¿›å…¥æ·±åº¦è¯Šç–—è¾…åŠ©æ¨¡å¼ã€‚è¯·ä¸¥æ ¼éµå¾ªä¸´åºŠæ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œ"
     "åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼ŒæŒ‰ä»¥ä¸‹é€»è¾‘é¡ºåºé€æ­¥æ¨å¯¼å¹¶å›ç­”é—®é¢˜ï¼š\n"
     "1. **æ ¸å¿ƒé—®é¢˜è¯†åˆ«**ï¼šæ˜ç¡®ç”¨æˆ·æ‰€é—®ç–¾ç—…çš„åç§°æˆ–æ ¸å¿ƒç—‡çŠ¶ã€‚\n"
     "2. **ç—…ç†æœºåˆ¶/ä¸­åŒ»ç—…æœº**ï¼šç®€è¿°ç°ä»£åŒ»å­¦çš„ç—…ç†ç”Ÿç†åŸºç¡€æˆ–ä¸­åŒ»çš„ç—…å› ç—…æœºï¼ˆå¦‚é£ã€æ¹¿ã€çƒ­ã€è¡€ç˜€ç­‰ï¼‰ã€‚\n"
     "3. **å…¸å‹ä¸´åºŠè¡¨ç°**ï¼šåˆ—å‡ºå…³é”®ä½“å¾ã€ç—‡çŠ¶ç‰¹ç‚¹åŠå¥½å‘éƒ¨ä½ã€‚\n"
     "4. **é‰´åˆ«è¯Šæ–­è¦ç‚¹**ï¼šæŒ‡å‡ºéœ€ä¸å“ªäº›å¸¸è§çš®è‚¤ç—…åŒºåˆ†ï¼Œå¹¶è¯´æ˜å…³é”®é‰´åˆ«ç‰¹å¾ã€‚\n"
     "5. **è¯Šç–—å»ºè®®**ï¼š\n"
     "   - è¥¿åŒ»ï¼šä¸€çº¿æ²»ç–—æ–¹æ¡ˆï¼ˆå¦‚å¤–ç”¨/ç³»ç»Ÿè¯ç‰©ï¼‰ï¼›\n"
     "   - ä¸­åŒ»ï¼šè¾¨è¯åˆ†å‹åŠå¯¹åº”æ²»æ³•æ–¹è¯ï¼ˆè‹¥èµ„æ–™æ”¯æŒï¼‰ï¼›\n"
     "   - ç”Ÿæ´»è°ƒæŠ¤ï¼šæ—¥å¸¸æ³¨æ„äº‹é¡¹æˆ–é¿å…è¯±å› ã€‚\n"
     "6. **çŸ¥è¯†è¾¹ç•Œè¯´æ˜**ï¼šè‹¥å‚è€ƒèµ„æ–™ä¸è¶³ä»¥è¦†ç›–ä¸Šè¿°ä»»ä¸€ç¯èŠ‚ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºâ€œç°æœ‰èµ„æ–™æœªæåŠXXéƒ¨åˆ†â€ã€‚\n\n"
     "è¦æ±‚ï¼š\n"
     "- ä½¿ç”¨è§„èŒƒåŒ»å­¦æœ¯è¯­ï¼Œä½†å¯¹å…³é”®æ¦‚å¿µï¼ˆå¦‚â€œTh17é€šè·¯â€â€œè¡€çƒ­è¯â€ï¼‰éœ€ç®€è¦è§£é‡Šï¼›\n"
     "- é€»è¾‘æ¸…æ™°ï¼Œåˆ†ç‚¹é™ˆè¿°ï¼Œé¿å…å†—é•¿æ®µè½ï¼›\n"
     "- ç»“å°¾æå‡ºä¸€ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„ä¸´åºŠé—®é¢˜ï¼ˆå¦‚æœºåˆ¶ã€æ²»ç–—éš¾ç‚¹æˆ–ä¸­è¥¿åŒ»ç»“åˆåˆ‡å…¥ç‚¹ï¼‰ã€‚"
    ),
    ("human", "å‚è€ƒèµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{question}")
])

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

basic_chain = ( basic_prompt
    | llm
    | StrOutputParser()
)

deep_chain = (deep_prompt
    | llm
    | StrOutputParser()
)

# 5. äº¤äº’å¾ªç¯
print("âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼è¯·è¾“å…¥æ‚¨çš„çš®è‚¤ç›¸å…³é—®é¢˜ï¼ˆè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼‰ï¼š")

# ç»Ÿä¸€é—®ç­”å…¥å£ 
def ask_with_mode(question: str, use_deep: bool):
    retrieved_docs = retriever.invoke(question)
    context = format_docs(retrieved_docs)
    
    # å°† context å’Œ question ä¼ ç»™é“¾
    input_data = {"context": context, "question": question}
    
    if use_deep:
        answer = deep_chain.invoke(input_data)
    else:
        answer = basic_chain.invoke(input_data)
    
    return {
        "result": answer,
        "source_documents": retrieved_docs
    }

# === ä¸»äº¤äº’å¾ªç¯ ===
while True:
    question = input("\nâ“ ä½ çš„é—®é¢˜: ").strip()
    if question.lower() in ["quit", "exit", "é€€å‡º"]:
        print("ğŸ‘‹ å†è§ï¼")
        break
    if not question:
        continue

    # ç¬¬ä¸€æ­¥ï¼šå…ˆé—®æ˜¯å¦æ·±åº¦æ€è€ƒ
    while True:
        deep_choice = input("ğŸ” æ˜¯å¦è¿›å…¥æ·±åº¦æ€è€ƒæ¨¡å¼ï¼Ÿ(y/n): ").strip().lower()
        if deep_choice in ['y', 'yes', 'æ˜¯']:
            use_deep = True
            break
        elif deep_choice in ['n', 'no', 'å¦', '']:
            use_deep = False
            break
        else:
            print("è¯·è¾“å…¥ y æˆ– n")

    try:
        print('æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å...')
        result = ask_with_mode(question, use_deep=use_deep)

        # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒæ ‡é¢˜
        mode_label = "ğŸ”¬ æ·±åº¦è§£æ" if use_deep else "ğŸ’¡ ç®€æ˜å›ç­”"
        print(f"\n{mode_label}ï¼š{result['result']}")
        
        # æ˜¾ç¤ºå‚è€ƒæ¥æºï¼ˆä¸¤ç§æ¨¡å¼éƒ½ä¿ç•™ï¼Œå¢å¼ºå¯ä¿¡åº¦ï¼‰
        print("\nğŸ“š éƒ¨åˆ†å‚è€ƒç‰‡æ®µï¼š")
        for i, doc in enumerate(result["source_documents"][:2], 1):
            print(f"  [{i}] {doc.page_content[:200]}...")

    except Exception as e:
        print(f"âŒ å‡ºç°é”™è¯¯: {e}")
# %%
